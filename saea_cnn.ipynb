{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate Assisted Evolutionary Algorithms for CNN Hyperparameter Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for running the EA and generating an evolved population. Visualizations are done in the `results.ipynb` notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import AlexNet, SmallCNN, get_loaders, train_model\n",
    "from es import *\n",
    "from surrogates import Surrogates\n",
    "from copy import deepcopy\n",
    "\n",
    "import oapackage\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Data for Surrogate Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a curated training population using hyperparameters close to default and recommended values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if saved file exists\n",
    "if os.path.exists('curated_training_population.pkl'):\n",
    "    with open('curated_training_population.pkl', 'rb') as f:\n",
    "        training_population1 = pickle.load(f)\n",
    "else:\n",
    "\n",
    "    # Curated hyperparameters\n",
    "    lrs = [0.01, 0.001, 0.0001]\n",
    "    momentums = [0.9, 0.99, 0]\n",
    "    weight_decays = [0.0005, 0.0001, 0]\n",
    "\n",
    "    # Create a list of hyperparameter combinations\n",
    "    hyperparams = []\n",
    "    for lr in lrs:\n",
    "        for momentum in momentums:\n",
    "            for weight_decay in weight_decays:\n",
    "                hyperparams.append((lr, momentum, weight_decay))\n",
    "\n",
    "    hyperparams = np.array(hyperparams)\n",
    "\n",
    "    # create training population from hyperparams\n",
    "    training_population1 = [Genome(x=hyperparam) for hyperparam in hyperparams]\n",
    "\n",
    "    # Update fitnesses using multiprocessing on full CNN  \n",
    "    with ProcessPoolExecutor(TRAIN_CONCURRENT) as executor:\n",
    "        results = list(tqdm(executor.map(fitness, training_population1), total=len(training_population1)))\n",
    "\n",
    "    for i in range(len(training_population1)):\n",
    "        training_population1[i].valid_acc = results[i][0]\n",
    "        training_population1[i].train_acc = results[i][1]\n",
    "        training_population1[i].train_loss = results[i][2]\n",
    "        training_population1[i].loss_target_fitness = results[i][3]\n",
    "\n",
    "    # save the population to a file\n",
    "    with open('curated_training_population.pkl', 'wb') as f:\n",
    "        pickle.dump(training_population1, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random training population using uniform distributions for each hyperparameter (via default Genome constructor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if saved file exists\n",
    "if os.path.exists('random_training_population.pkl'):\n",
    "    with open('random_training_population.pkl', 'rb') as f:\n",
    "        training_population2 = pickle.load(f)\n",
    "else:\n",
    "    \n",
    "    # Initialize population creation\n",
    "    training_population2 = [Genome() for _ in range(100)]\n",
    "\n",
    "    # Update fitnesses using multiprocessing on full CNN   \n",
    "    with ProcessPoolExecutor(TRAIN_CONCURRENT) as executor:\n",
    "        results = list(tqdm(executor.map(fitness, training_population2), total=len(training_population2)))\n",
    "\n",
    "    # update the population with the results\n",
    "    for i in range(len(training_population2)):\n",
    "        training_population2[i].valid_acc = results[i][0]\n",
    "        training_population2[i].train_acc = results[i][1]\n",
    "        training_population2[i].train_loss = results[i][2]\n",
    "        training_population2[i].loss_target_fitness = results[i][3]\n",
    "\n",
    "    # save the population to a file\n",
    "    with open('random_training_population.pkl', 'wb') as f:\n",
    "        pickle.dump(training_population2, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the two populations\n",
    "population = training_population1 + training_population2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean population validation accuracy\n",
    "mean_valid_acc = np.mean([genome.valid_acc for genome in population])\n",
    "print('Mean population validation accuracy: {}'.format(mean_valid_acc))\n",
    "\n",
    "# mean population training accuracy\n",
    "mean_train_acc = np.mean([genome.train_acc for genome in population])\n",
    "print('Mean population training accuracy: {}'.format(mean_train_acc))\n",
    "\n",
    "# mean population training loss\n",
    "mean_train_loss = np.mean([genome.train_loss for genome in population])\n",
    "print('Mean population training loss: {}'.format(mean_train_loss))\n",
    "\n",
    "# mean population loss target fitness\n",
    "mean_loss_target_fitness = np.mean([genome.loss_target_fitness for genome in population])\n",
    "print('Mean population loss target fitness: {}'.format(mean_loss_target_fitness))\n",
    "\n",
    "# max population validation accuracy\n",
    "max_valid_acc = np.max([genome.valid_acc for genome in population])\n",
    "print('Max population validation accuracy: {}'.format(max_valid_acc))\n",
    "\n",
    "# max population training accuracy\n",
    "max_train_acc = np.max([genome.train_acc for genome in population])\n",
    "print('Max population training accuracy: {}'.format(max_train_acc))\n",
    "\n",
    "# max population training loss\n",
    "max_train_loss = np.max([genome.train_loss for genome in population])\n",
    "print('Max population training loss: {}'.format(max_train_loss))\n",
    "\n",
    "# max population loss target fitness\n",
    "max_loss_target_fitness = np.max([genome.loss_target_fitness for genome in population])\n",
    "print('Max population loss target fitness: {}'.format(max_loss_target_fitness))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find and visualize the pareto front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the pareto front\n",
    "pareto=oapackage.ParetoDoubleLong()\n",
    "\n",
    "for ii in range(0, len(population)):\n",
    "    w=oapackage.doubleVector((population[ii].valid_acc, population[ii].loss_target_fitness))\n",
    "    pareto.addvalue(w, ii)\n",
    "# show the pareto front size \n",
    "pareto.show(verbose=1)\n",
    "\n",
    "# show the pareto front visualization\n",
    "pareto_front = [population[i] for i in pareto.allindices()]\n",
    "h=plt.plot([genome.valid_acc for genome in population], [genome.loss_target_fitness for genome in population], '.b', markersize=16, label='Non Pareto-optimal')\n",
    "h=plt.plot([genome.valid_acc for genome in pareto_front], [genome.loss_target_fitness for genome in pareto_front], '.r', markersize=16, label='Pareto-optimal')\n",
    "plt.xlabel('Validation Accuracy', fontsize=16)\n",
    "plt.ylabel('Loss Target Fitness', fontsize=16)\n",
    "_=plt.legend(loc=3, numpoints=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train Surrogate Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our experiments, we will use a support vector regressor (with RBF kernel), a gradient boosting regressor, a kernel ridge regressor (with RBF kernels) as our surrogate models. A voting ensemble will be used to combine the predictions of the three models. Because we have two objectives, we will train two models for each surrogate model type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the surrogates model\n",
    "surrogates = Surrogates()\n",
    "\n",
    "# train the surrogates model\n",
    "X = np.array([genome.x for genome in population])\n",
    "y = np.array([[genome.valid_acc, genome.loss_target_fitness] for genome in population])\n",
    "\n",
    "surrogates.train(X, y, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save full trained population to a file. This file will updated anytime a new model is fully trained\n",
    "fully_trained_population = []\n",
    "fully_trained_population.append(deepcopy(population))\n",
    "with open('fully_trained_population.pkl', 'wb') as f:\n",
    "    pickle.dump(fully_trained_population, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolutionary Algorithm: Initial Population and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters come from es.py\n",
    "mu=MU\n",
    "lambda_=LAMBDA\n",
    "sigma_crossover_rate=SIGMA_CROSSOVER_RATE\n",
    "x_crossover_rate=X_CROSSOVER_RATE\n",
    "sigma_mutation_rate=SIGMA_MUTATION_RATE\n",
    "x_mutation_rate=X_MUTATION_RATE\n",
    "max_generations=MAX_GENERATIONS\n",
    "converge_threshold=CONVERGE_THRESHOLD\n",
    "display_stats=True\n",
    "retrain_frequency=RETRAIN_FREQUENCY\n",
    "train_concurrent=TRAIN_CONCURRENT\n",
    "\n",
    "# Initialize population creation\n",
    "population = [Genome() for _ in range(mu)]\n",
    "\n",
    "# Update fitnesses via surrogates model\n",
    "X = np.array([genome.x for genome in population])\n",
    "y_pred = surrogates.predict(X)\n",
    "for i in range(len(population)):\n",
    "    population[i].valid_acc = y_pred[i][0]\n",
    "    population[i].loss_target_fitness = y_pred[i][1]\n",
    "\n",
    "\n",
    "# Initialize lists to store generational statistics\n",
    "generational_valid_acc_max = []\n",
    "generational_valid_acc_min = []\n",
    "generational_valid_acc_mean = []\n",
    "\n",
    "generational_loss_target_max = []\n",
    "generational_loss_target_min = []\n",
    "generational_loss_target_mean = []\n",
    "\n",
    "generational_pareto_fronts = []\n",
    "generational_diversity = []\n",
    "\n",
    "generational_surrogate_mae = []\n",
    "\n",
    "# get statistics about initial generation\n",
    "vaild_accs = [genome.valid_acc for genome in population]\n",
    "loss_targets = [genome.loss_target_fitness for genome in population]\n",
    "\n",
    "generational_valid_acc_max.append(max(vaild_accs))\n",
    "generational_valid_acc_min.append(min(vaild_accs))\n",
    "generational_valid_acc_mean.append(np.mean(vaild_accs))\n",
    "\n",
    "generational_loss_target_max.append(max(loss_targets))\n",
    "generational_loss_target_min.append(min(loss_targets))\n",
    "generational_loss_target_mean.append(np.mean(loss_targets))\n",
    "\n",
    "generational_pareto_fronts.append(pareto_front)\n",
    "generational_diversity.append(get_population_diversity(population))\n",
    "\n",
    "generational_surrogate_mae.append(surrogates.get_mae())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolutionary Algorithm: Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gen_count in range(max_generations):\n",
    "    # print generational stats\n",
    "    if display_stats:\n",
    "        print(f'{gen_count} {generational_valid_acc_min[-1]:.4f} {generational_valid_acc_mean[-1]:.4f} {generational_valid_acc_max[-1]:.4f}',\n",
    "                f'{generational_loss_target_min[-1]:.4f} {generational_loss_target_mean[-1]:.4f} {generational_loss_target_max[-1]:.4f} {generational_diversity[-1]:.4f}')\n",
    "        \n",
    "    # container to store new generation\n",
    "    new_population = []\n",
    "\n",
    "    # loop to create new generation of size lambda_\n",
    "    for i in range(lambda_ // 2):\n",
    "        # Uniform random parent selection\n",
    "        parent_1_idx = np.random.randint(0, len(population))\n",
    "        parent_2_idx = np.random.randint(0, len(population))\n",
    "\n",
    "        parent_1 = deepcopy(population[parent_1_idx])\n",
    "        parent_2 = deepcopy(population[parent_2_idx])\n",
    "\n",
    "        # sigma crossover (intermediate recombination)\n",
    "        sigma_crossover(parent_1, parent_2, crossover_rate=sigma_crossover_rate)\n",
    "\n",
    "        # sigma mutation \n",
    "        sigma_mutate(parent_1, mutation_rate=sigma_mutation_rate)\n",
    "        sigma_mutate(parent_2, mutation_rate=sigma_mutation_rate)\n",
    "        \n",
    "        # x crossover intermediate recombination\n",
    "        crossover(parent_1, parent_2, crossover_rate=x_crossover_rate)\n",
    "\n",
    "        # x mutation\n",
    "        mutate(parent_1, mutation_rate=x_mutation_rate)\n",
    "        mutate(parent_2, mutation_rate=x_mutation_rate)\n",
    "            \n",
    "        # add to new generation\n",
    "        new_population.append(parent_1)\n",
    "        new_population.append(parent_2)\n",
    "\n",
    "    # limit genome X values to be within bounds\n",
    "    for genome in new_population:\n",
    "        genome.x[0] = np.clip(genome.x[0], 0, 0.2)\n",
    "        genome.x[1] = np.clip(genome.x[1], 0, 0.999)\n",
    "        genome.x[2] = np.clip(genome.x[2], 0, 0.05)\n",
    "\n",
    "    # Update fitnesses via surrogates model\n",
    "    X = np.array([genome.x for genome in new_population])\n",
    "    y_pred = surrogates.predict(X)\n",
    "    for i in range(len(new_population)):\n",
    "        new_population[i].valid_acc = y_pred[i][0]\n",
    "        new_population[i].loss_target_fitness = y_pred[i][1]\n",
    "    \n",
    "    # add new generation to population\n",
    "    population.extend(new_population)\n",
    "\n",
    "    # find the pareto front\n",
    "    pareto=oapackage.ParetoDoubleLong()\n",
    "    for ii in range(0, len(population)):\n",
    "        w=oapackage.doubleVector((population[ii].valid_acc, population[ii].loss_target_fitness))\n",
    "        pareto.addvalue(w, ii)\n",
    "\n",
    "    pareto_front = [population[ii] for ii in pareto.allindices()]\n",
    "\n",
    "    # chose the top mu/2 from the pareto front based on crowding distance\n",
    "    X = np.array([genome.x for genome in pareto_front])\n",
    "    distances = CrowdingDist(X)\n",
    "    new_front = []\n",
    "    sorted_indexes = np.argsort(distances)[::-1]\n",
    "    for idx in sorted_indexes[:int(mu / 2)]:\n",
    "        new_front.append(pareto_front[idx])\n",
    "    pareto_front = new_front\n",
    "\n",
    "    # create a new population consisting of the pareto front, the top validation accuracy, and the top loss target fitness\n",
    "    new_population = [genome for genome in pareto_front]\n",
    "\n",
    "    sorted1 = sorted(population, key=lambda genome: genome.valid_acc, reverse=True)\n",
    "    sorted2 = sorted(population, key=lambda genome: genome.loss_target_fitness, reverse=True)\n",
    "    while len(new_population) < mu-1:\n",
    "        new_population.append(sorted1.pop(0))\n",
    "        new_population.append(sorted2.pop(0))\n",
    "        new_population = list(set(new_population))\n",
    "\n",
    "    population = new_population\n",
    "\n",
    "    # every retrain_frequency generations, retrain the surrogates model using full CNN training\n",
    "    if gen_count > 0 and gen_count % retrain_frequency == 0:\n",
    "\n",
    "        # Update fitnesses using multiprocessing on full CNN  \n",
    "        with ProcessPoolExecutor(train_concurrent) as executor:\n",
    "            results = list(tqdm(executor.map(fitness, population), total=len(population)))\n",
    "        \n",
    "        # update fitnesses with results\n",
    "        for i in range(len(population)):\n",
    "            population[i].valid_acc = results[i][0]\n",
    "            population[i].loss_target_fitness = results[i][1]\n",
    "        \n",
    "        # update surrogates model\n",
    "        X = np.array([genome.x for genome in population])\n",
    "        y = np.array([[genome.valid_acc, genome.loss_target_fitness] for genome in population])\n",
    "        surrogates.train(X, y)\n",
    "\n",
    "        # update pareto front\n",
    "        pareto=oapackage.ParetoDoubleLong()\n",
    "        for ii in range(0, len(population)):\n",
    "            w=oapackage.doubleVector((population[ii].valid_acc, population[ii].loss_target_fitness))\n",
    "            pareto.addvalue(w, ii)\n",
    "\n",
    "        pareto_front = [population[ii] for ii in pareto.allindices()]\n",
    "\n",
    "        # save fully trained population\n",
    "        fully_trained_population.append(deepcopy(population))\n",
    "        with open('fully_trained_population.pkl', 'wb') as f:\n",
    "            pickle.dump(fully_trained_population, f) \n",
    "\n",
    "        # Pickle generational statistics and save to file\n",
    "        with open(f'generational_stats.pkl', 'wb') as f: \n",
    "            pickle.dump((generational_valid_acc_max, generational_valid_acc_min, generational_valid_acc_mean,\n",
    "                        generational_loss_target_max, generational_loss_target_min, generational_loss_target_mean,\n",
    "                        generational_pareto_fronts,  generational_diversity, generational_surrogate_mae), f)\n",
    "\n",
    "\n",
    "    # get statistics about new population\n",
    "    vaild_accs = [genome.valid_acc for genome in population]\n",
    "    loss_targets = [genome.loss_target_fitness for genome in population]\n",
    "\n",
    "    generational_valid_acc_max.append(max(vaild_accs))\n",
    "    generational_valid_acc_min.append(min(vaild_accs))\n",
    "    generational_valid_acc_mean.append(np.mean(vaild_accs))\n",
    "\n",
    "    generational_loss_target_max.append(max(loss_targets))\n",
    "    generational_loss_target_min.append(min(loss_targets))\n",
    "    generational_loss_target_mean.append(np.mean(loss_targets))\n",
    "\n",
    "    generational_pareto_fronts.append(pareto_front)\n",
    "    generational_diversity.append(get_population_diversity(population))\n",
    "\n",
    "    generational_surrogate_mae.append(surrogates.get_mae())\n",
    "\n",
    "    # Check for termination conditions\n",
    "    # (1) - terminate if generational diversity is below threshold\n",
    "    # if generational_diversity[-1] < converge_threshold :\n",
    "    #     if display_stats:\n",
    "    #         print(f'Population has converged with generational diversity below threshold of {converge_threshold }')\n",
    "    #     break\n",
    "\n",
    "    # (2) - terminate after max_generations (display message)\n",
    "    if gen_count == max_generations - 1 and display_stats:\n",
    "        print('The maximum number of generations has been reached')\n",
    "\n",
    "# Pickle generational statistics and save to file\n",
    "with open(f'generational_stats.pkl', 'wb') as f: \n",
    "    pickle.dump((generational_valid_acc_max, generational_valid_acc_min, generational_valid_acc_mean,\n",
    "                generational_loss_target_max, generational_loss_target_min, generational_loss_target_mean,\n",
    "                generational_pareto_fronts,  generational_diversity, generational_surrogate_mae), f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pareto_front)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for pareto_front in generational_pareto_fronts:\n",
    "#     plt.figure(figsize=(8,8))\n",
    "#     plt.plot([genome.valid_acc for genome in pareto_front], [genome.loss_target_fitness for genome in pareto_front], '.r', markersize=16, label='Pareto-optimal')\n",
    "#     plt.xlabel('Validation Accuracy', fontsize=16)\n",
    "#     plt.ylabel('Loss Target Fitness', fontsize=16)\n",
    "#     _=plt.legend(loc=3, numpoints=1)\n",
    "pareto_front = generational_pareto_fronts[-801]\n",
    "plt.plot([genome.valid_acc for genome in population], [genome.loss_target_fitness for genome in population], '.b', markersize=16, label='Non Pareto-optimal')\n",
    "plt.plot([genome.valid_acc for genome in pareto_front], [genome.loss_target_fitness for genome in pareto_front], '.r', markersize=16, label='Pareto-optimal')\n",
    "plt.xlabel('Validation Accuracy', fontsize=16)\n",
    "plt.ylabel('Loss Target Fitness', fontsize=16)\n",
    "_=plt.legend(loc=3, numpoints=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph generational statistics\n",
    "plt.figure()\n",
    "plt.plot(generational_valid_acc_max, label='Max')\n",
    "plt.plot(generational_valid_acc_min, label='Min')\n",
    "plt.plot(generational_valid_acc_mean, label='Mean')\n",
    "plt.xlabel('Generation', fontsize=16)\n",
    "plt.ylabel('Validation Accuracy', fontsize=16)\n",
    "plt.legend(loc=4, numpoints=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(generational_loss_target_max, label='Max')\n",
    "plt.plot(generational_loss_target_min, label='Min')\n",
    "plt.plot(generational_loss_target_mean, label='Mean')\n",
    "plt.xlabel('Generation', fontsize=16)\n",
    "plt.ylabel('Loss Target Fitness', fontsize=16)\n",
    "plt.legend(loc=4, numpoints=1)\n",
    "\n",
    "\n",
    "# graph generational diversity\n",
    "plt.figure()\n",
    "plt.plot(generational_diversity)\n",
    "plt.xlabel('Generation', fontsize=16)\n",
    "plt.ylabel('Diversity', fontsize=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot surrogate MAE\n",
    "plt.figure()\n",
    "plt.plot([mae['msvr_rbf'] for mae in generational_surrogate_mae])\n",
    "plt.figure()\n",
    "plt.plot([mae['msvr_laplace'] for mae in generational_surrogate_mae])\n",
    "plt.figure()\n",
    "plt.plot([mae['rfr'] for mae in generational_surrogate_mae])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
